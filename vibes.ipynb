{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-21T07:44:23.818731Z"
    }
   },
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ccxt\n",
    "import vectorbt as vbt\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "import concurrent.futures\n",
    "import torch\n",
    "\n",
    "# Use Tensor Cores for faster ops\n",
    "torch.set_float32_matmul_precision('high')\n",
    "print(\"Starting DeepAR backtest pipeline with enhanced logging...\")\n",
    "\n",
    "# --- Config ---\n",
    "DATA_DIR = \"data/binance_backtesting\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "prediction_length = 10\n",
    "context_length = 120\n",
    "init_cash = 100000.0\n",
    "max_position_size = 0.10  # Maximum 10% per trade\n",
    "stop_loss = 0.03      # 3% stop-loss\n",
    "take_profit = 0.07    # 7% take-profit\n",
    "max_workers = 32      # threads for signal generation\n",
    "fetch_workers = 8     # threads for fetching\n",
    "\n",
    "# Bollinger Bands params\n",
    "bb_window = 20\n",
    "bb_std_factor = 2\n",
    "\n",
    "# VMA params\n",
    "vma_window = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Step 1: Parallel fetch & cache data ---\n",
    "print(\"[Step 1] Fetching OHLCV in parallel...\")\n",
    "exchange = ccxt.binanceus()\n",
    "exchange.load_markets()\n",
    "usdt_symbols = [s for s in exchange.symbols if s.endswith('/USDT')]\n",
    "if not usdt_symbols:\n",
    "    raise RuntimeError(\"No USDT symbols found\")\n",
    "if len(usdt_symbols) > 10:\n",
    "    print(f\"    Found {len(usdt_symbols)} symbols; fetching first 10 for demo.\")\n",
    "    usdt_symbols = usdt_symbols[:10]\n",
    "\n",
    "six_weeks_ms = 42 * 24 * 60 * 60 * 1000\n",
    "end_time = exchange.milliseconds()\n",
    "start_time = end_time - six_weeks_ms\n",
    "\n",
    "def fetch_and_cache(symbol: str):\n",
    "    print(f\"    [Fetch] {symbol}\")\n",
    "    fname = symbol.replace('/', '_') + '.parquet'\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"      Cached, skipping\")\n",
    "        return\n",
    "    all_ohlcv = []\n",
    "    since_ms = start_time\n",
    "    while True:\n",
    "        chunk = exchange.fetch_ohlcv(symbol, '1m', since=since_ms, limit=1000)\n",
    "        if not chunk:\n",
    "            break\n",
    "        all_ohlcv.extend(chunk)\n",
    "        since_ms = chunk[-1][0] + 60_000\n",
    "        if len(chunk) < 1000:\n",
    "            break\n",
    "    df = pd.DataFrame(all_ohlcv, columns=['timestamp','open','high','low','close','volume'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df['symbol'] = symbol\n",
    "    df.to_parquet(path)\n",
    "    print(f\"      Saved {len(df)} rows for {symbol}\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=fetch_workers) as ex:\n",
    "    ex.map(fetch_and_cache, usdt_symbols)\n",
    "print(\"[Step 1] Fetch complete\\n\")\n",
    "\n",
    "# --- Step 1b: Load data + compute indicators ---\n",
    "print(\"[Step 1b] Loading data and computing RSI, Bollinger Bands, VMA...\")\n",
    "market_data = {}\n",
    "for path in glob.glob(os.path.join(DATA_DIR, '*.parquet')):\n",
    "    sym = os.path.basename(path).replace('.parquet','').replace('_','/')\n",
    "    df = pd.read_parquet(path)\n",
    "    \n",
    "    # Handle the case where timestamp might already be an index or have a different name\n",
    "    if 'timestamp' in df.columns:\n",
    "        df = df.set_index('timestamp')\n",
    "    elif df.index.name != 'timestamp' and df.index.dtype != 'datetime64[ns]':\n",
    "        # Try to find a datetime column to use as index\n",
    "        datetime_cols = [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col])]\n",
    "        if datetime_cols:\n",
    "            df = df.set_index(datetime_cols[0])\n",
    "        else:\n",
    "            print(f\"Warning: No timestamp column found for {sym}, using default index\")\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    ma_up = up.ewm(span=14, adjust=False).mean()\n",
    "    ma_down = down.ewm(span=14, adjust=False).mean()\n",
    "    df['rsi'] = 100 - (100/(1 + ma_up/(ma_down + 1e-9)))\n",
    "    # Bollinger Bands\n",
    "    df['bb_mid'] = df['close'].rolling(bb_window).mean()\n",
    "    std = df['close'].rolling(bb_window).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + bb_std_factor * std\n",
    "    df['bb_lower'] = df['bb_mid'] - bb_std_factor * std\n",
    "    # Volume Moving Average\n",
    "    df['vma'] = df['volume'].rolling(vma_window).mean()\n",
    "    df.bfill(inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    market_data[sym] = df\n",
    "print(f\"    Prepared indicators for {len(market_data)} symbols\\n\")\n",
    "\n",
    "# --- Step 2: Train DeepAR with sliding windows ---\n",
    "print(\"[Step 2] Preparing sliding-window training data...\")\n",
    "train_items = []\n",
    "window_stride = prediction_length\n",
    "for symbol, df in market_data.items():\n",
    "    n = len(df)\n",
    "    cutoff = n - (14 * 24 * 60)\n",
    "    if cutoff <= context_length + prediction_length:\n",
    "        print(f\"    Skipping {symbol}: insufficient data (n={n})\")\n",
    "        continue\n",
    "    for end_i in range(context_length + prediction_length, cutoff, window_stride):\n",
    "        start_i = end_i - (context_length + prediction_length)\n",
    "        train_items.append({\n",
    "            'target': df['close'].iloc[start_i:end_i].values,\n",
    "            'feat_dynamic_real': [\n",
    "                df['volume'].iloc[start_i:end_i].values,\n",
    "                df['rsi'].iloc[start_i:end_i].values,\n",
    "                df['bb_upper'].iloc[start_i:end_i].values,\n",
    "                df['bb_lower'].iloc[start_i:end_i].values,\n",
    "                df['vma'].iloc[start_i:end_i].values\n",
    "            ],\n",
    "            'start': df.index[start_i],\n",
    "            'item_id': symbol\n",
    "        })\n",
    "print(f\"    Training on {len(train_items)} windows\")\n",
    "training_ds = ListDataset(train_items, freq='T')\n",
    "estimator = DeepAREstimator(\n",
    "    freq='T',\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    num_layers=3,\n",
    "    hidden_size=100,\n",
    "    batch_size=1024,\n",
    "    trainer_kwargs={\n",
    "        'max_epochs': 6,\n",
    "        'accelerator': 'gpu' if device.type=='cuda' else 'cpu',\n",
    "        'devices': 1,\n",
    "        'precision': 16,\n",
    "        'logger': False,\n",
    "        'limit_val_batches': 0,\n",
    "    }\n",
    ")\n",
    "predictor = estimator.train(training_data=training_ds)\n",
    "if hasattr(predictor, 'network'):\n",
    "    predictor.network.to(device)\n",
    "    predictor.network.eval()\n",
    "    predictor.network.half()\n",
    "print(\"    Training complete\\n\")\n",
    "\n",
    "# --- Step 3: Precompute and inference separation ---\n",
    "print(\"[Step 3] Precomputing sliding windows on CPU...\")\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "windows_data = {}\n",
    "for symbol, df in market_data.items():\n",
    "    n = len(df)\n",
    "    test_win = 14 * 24 * 60\n",
    "    start_i = max(context_length, n - test_win)\n",
    "    end_i   = n - prediction_length\n",
    "    if end_i - start_i <= 0:\n",
    "        continue\n",
    "    closes_np = sliding_window_view(df['close'].values, context_length)[start_i-context_length:end_i-context_length]\n",
    "    vols_np   = sliding_window_view(df['volume'].values, context_length)[start_i-context_length:end_i-context_length]\n",
    "    rsi_np    = sliding_window_view(df['rsi'].values, context_length)[start_i-context_length:end_i-context_length]\n",
    "    bb_up_np  = sliding_window_view(df['bb_upper'].values, context_length)[start_i-context_length:end_i-context_length]\n",
    "    bb_low_np = sliding_window_view(df['bb_lower'].values, context_length)[start_i-context_length:end_i-context_length]\n",
    "    vma_np    = sliding_window_view(df['vma'].values, context_length)[start_i-context_length:end_i-context_length]\n",
    "    idxs = np.arange(start_i, end_i)\n",
    "    windows_data[symbol] = (closes_np, vols_np, rsi_np, bb_up_np, bb_low_np, vma_np, idxs)\n",
    "print(\"[Step 3] CPU aggregation complete\")\n",
    "\n",
    "print(\"[Step 3] Preparing global dataset for GPU inference...\")\n",
    "all_ds_items, index_map = [], []\n",
    "for symbol, (closes_np, vols_np, rsi_np, bb_up_np, bb_low_np, vma_np, idxs) in windows_data.items():\n",
    "    for close_win, vol_win, rsi_win, bb_up_win, bb_low_win, vma_win, idx in zip(\n",
    "        closes_np, vols_np, rsi_np, bb_up_np, bb_low_np, vma_np, idxs\n",
    "    ):\n",
    "        all_ds_items.append({\n",
    "            'target': close_win.tolist(),\n",
    "            'feat_dynamic_real': [\n",
    "                vol_win.tolist(),\n",
    "                rsi_win.tolist(),\n",
    "                bb_up_win.tolist(),\n",
    "                bb_low_win.tolist(),\n",
    "                vma_win.tolist()\n",
    "            ],\n",
    "            'start': market_data[symbol].index[idx-context_length]\n",
    "        })\n",
    "        index_map.append((symbol, idx))\n",
    "print(f\"    Aggregated {len(all_ds_items)} windows into one dataset\")\n",
    "\n",
    "dataset = ListDataset(all_ds_items, freq='T')\n",
    "print(\"[Step 3] Running inference on GPUâ€¦\")\n",
    "with torch.amp.autocast(device.type, dtype=torch.float16):\n",
    "    forecasts = list(predictor.predict(dataset, num_samples=1))\n",
    "\n",
    "# --- Step 4: Map signals and backtest ---\n",
    "price_df = pd.DataFrame({s: market_data[s]['close'] for s in market_data}).dropna(axis=1)\n",
    "n = min(map(len, price_df.values.T))\n",
    "price_df = price_df.iloc[-n:]\n",
    "symbols = price_df.columns.tolist()\n",
    "entries = {sym: np.zeros(len(price_df), dtype=bool) for sym in symbols}\n",
    "exits   = {sym: np.zeros(len(price_df), dtype=bool) for sym in symbols}\n",
    "\n",
    "# Store prediction confidence for dynamic position sizing\n",
    "confidence_scores = {sym: np.zeros(len(price_df)) for sym in symbols}\n",
    "\n",
    "for (symbol, idx), fc in zip(index_map, forecasts):\n",
    "    if symbol not in symbols:\n",
    "        continue\n",
    "    rel_idx = idx - (len(market_data[symbol]) - len(price_df))\n",
    "    if not 0 <= rel_idx < len(price_df):\n",
    "        continue\n",
    "    price = price_df[symbol].values[rel_idx]\n",
    "    mean_pred = float(fc.mean[-1])\n",
    "    pred_percentile = float(fc.quantile(0.975)[-1])\n",
    "    \n",
    "    # Calculate prediction confidence based on the mean prediction relative to current price\n",
    "    # Normalize confidence between 0.0 and 1.0\n",
    "    predicted_return = (mean_pred / price) - 1.0\n",
    "    \n",
    "    # Only consider positive expected returns\n",
    "    if predicted_return > 0.05:  # 5% threshold for entry\n",
    "        entries[symbol][rel_idx] = True\n",
    "        \n",
    "        # Store confidence score based on expected return (capped at 0.5 for safety)\n",
    "        confidence = min(predicted_return, 0.5)\n",
    "        confidence_scores[symbol][rel_idx] = confidence\n",
    "        \n",
    "        exit_point = rel_idx + prediction_length\n",
    "        if exit_point < len(price_df):\n",
    "            exits[symbol][exit_point] = True\n",
    "\n",
    "print(\"[Step 4] Calculating dynamic position sizes based on prediction confidence...\")\n",
    "# Create dynamic position sizing based on prediction confidence\n",
    "position_sizes = {}\n",
    "for symbol in symbols:\n",
    "    # Scale the confidence scores to position sizes between 0.01 (1%) and max_position_size (10%)\n",
    "    # Higher confidence = larger position size, but never exceeding max_position_size\n",
    "    position_sizes[symbol] = np.zeros(len(price_df))\n",
    "    for i in range(len(price_df)):\n",
    "        if entries[symbol][i]:\n",
    "            # Calculate position size: min position size (1%) + scaled confidence (up to 9% more)\n",
    "            # This ensures position size is between 1% and max_position_size\n",
    "            position_sizes[symbol][i] = 0.01 + (confidence_scores[symbol][i] * (max_position_size - 0.01))\n",
    "            # Ensure we don't exceed max position size\n",
    "            position_sizes[symbol][i] = min(position_sizes[symbol][i], max_position_size)\n",
    "\n",
    "# Convert position sizes to DataFrame\n",
    "position_size_df = pd.DataFrame(position_sizes, index=price_df.index)\n",
    "\n",
    "print(\"[Step 4] Backtesting with vectorbt with dynamic sizing, SL/TP...\")\n",
    "entries_df = pd.DataFrame(entries, index=price_df.index)\n",
    "exits_df   = pd.DataFrame(exits,   index=price_df.index)\n",
    "\n",
    "# Create portfolio with dynamic position sizing\n",
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=price_df,\n",
    "    entries=entries_df,\n",
    "    exits=exits_df,\n",
    "    init_cash=init_cash,\n",
    "    fees=0.001,\n",
    "    slippage=0.001,\n",
    "    size=position_size_df,  # Now using the dynamic position sizes dataframe\n",
    "    size_type='Percent',\n",
    "    cash_sharing=True\n",
    ")\n",
    "print(\"    Dynamic position sizing backtest complete\")\n",
    "\n",
    "# Apply stop-loss and take-profit\n",
    "print(\"    Applying stop-loss and take-profit overlays...\")\n",
    "try:\n",
    "    pf = pf.apply_stop_loss(stop_loss, sl_stop_type='percent')\n",
    "    pf = pf.apply_take_profit(take_profit, tp_stop_type='percent')\n",
    "    print(\"    Stop-loss and take-profit applied\")\n",
    "except AttributeError:\n",
    "    print(\"    Stop-loss/take-profit methods not available in this VectorBT version. Please upgrade to v0.26+ to use overlays.\")\n",
    "\n",
    "print(\"    Final backtest ready    Backtest complete\")\n",
    "stats = pf.stats()\n",
    "print(stats)\n",
    "stats.to_csv(os.path.join(DATA_DIR, \"backtest_stats.csv\"))\n",
    "\n",
    "# Save position sizing statistics\n",
    "position_size_stats = {\n",
    "    'mean': position_size_df.mean(),\n",
    "    'median': position_size_df.median(),\n",
    "    'min': position_size_df.min(),\n",
    "    'max': position_size_df.max(),\n",
    "    'std': position_size_df.std()\n",
    "}\n",
    "pd.DataFrame(position_size_stats).to_csv(os.path.join(DATA_DIR, \"position_size_stats.csv\"))\n",
    "\n",
    "# Create plots\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot overall performance\n",
    "pf.plot(ax=ax1)\n",
    "ax1.set_title('Portfolio Performance')\n",
    "\n",
    "# Plot position sizes over time\n",
    "for symbol in symbols:\n",
    "    sizes = position_size_df[symbol][position_size_df[symbol] > 0]\n",
    "    if len(sizes) > 0:\n",
    "        ax2.scatter(sizes.index, sizes.values, label=symbol, alpha=0.7)\n",
    "ax2.set_ylim(0, max_position_size * 1.1)\n",
    "ax2.set_title('Dynamic Position Sizes Over Time')\n",
    "ax2.set_ylabel('Position Size (%)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATA_DIR, \"backtest_results.png\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"Pipeline finished\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeepAR backtest pipeline with enhanced logging...\n",
      "Using device: cuda\n",
      "[Step 1] Fetching OHLCV in parallel...\n",
      "    Found 192 symbols; fetching first 10 for demo.\n",
      "    [Fetch] 1INCH/USDT\n",
      "      Cached, skipping\n",
      "    [Fetch] AAVE/USDT\n",
      "      Cached, skipping\n",
      "    [Fetch] ACH/USDT\n",
      "    [Fetch] ADA/USDT\n",
      "      Cached, skipping\n",
      "    [Fetch] ADX/USDT\n",
      "      Cached, skipping\n",
      "      Cached, skipping\n",
      "    [Fetch] AIXBT/USDT\n",
      "    [Fetch] ALGO/USDT\n",
      "    [Fetch] ALICE/USDT\n",
      "      Cached, skipping\n",
      "    [Fetch] ALPINE/USDT\n",
      "      Cached, skipping\n",
      "      Cached, skipping\n",
      "      Cached, skipping\n",
      "    [Fetch] ANKR/USDT\n",
      "      Cached, skipping\n",
      "[Step 1] Fetch complete\n",
      "\n",
      "[Step 1b] Loading data and computing RSI, Bollinger Bands, VMA...\n",
      "    Prepared indicators for 192 symbols\n",
      "\n",
      "[Step 2] Preparing sliding-window training data...\n",
      "    Skipping AIXBT/USDT: insufficient data (n=18373)\n",
      "    Skipping ANT/USDT: insufficient data (n=0)\n",
      "    Skipping BOND/USDT: insufficient data (n=0)\n",
      "    Skipping BUSD/USDT: insufficient data (n=0)\n",
      "    Skipping CUDOS/USDT: insufficient data (n=0)\n",
      "    Skipping FLOKI8/USDT: insufficient data (n=0)\n",
      "    Skipping GAL/USDT: insufficient data (n=0)\n",
      "    Skipping HNT/USDT: insufficient data (n=0)\n",
      "    Skipping JASMY/USDT: insufficient data (n=0)\n",
      "    Skipping KAITO/USDT: insufficient data (n=8315)\n",
      "    Skipping OMG/USDT: insufficient data (n=0)\n",
      "    Skipping POLY/USDT: insufficient data (n=0)\n",
      "    Skipping RNDR/USDT: insufficient data (n=0)\n",
      "    Skipping SPELL/USDT: insufficient data (n=0)\n",
      "    Skipping SRM/USDT: insufficient data (n=0)\n",
      "    Skipping TRX/USDT: insufficient data (n=0)\n",
      "    Skipping TUSD/USDT: insufficient data (n=0)\n",
      "    Skipping UST/USDT: insufficient data (n=0)\n",
      "    Skipping WAVES/USDT: insufficient data (n=0)\n",
      "    Training on 4499624 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saber\\anaconda3\\Lib\\site-packages\\lightning\\fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\saber\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "C:\\Users\\saber\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\saber\\OneDrive\\Documents\\GitHub\\CryptoBot4Dummies\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode  | In sizes                                                      | Out sizes   \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 215 K  | train | [[1, 1], [1, 1], [1, 301, 6], [1, 301], [1, 301], [1, 10, 6]] | [1, 100, 10]\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "215 K     Trainable params\n",
      "0         Non-trainable params\n",
      "215 K     Total params\n",
      "0.860     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5952f989c66d4f36bfe0a2198fc99fde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 0.68300 (best 0.68300), saving model to 'C:\\\\Users\\\\saber\\\\OneDrive\\\\Documents\\\\GitHub\\\\CryptoBot4Dummies\\\\checkpoints\\\\epoch=0-step=50-v33.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'train_loss' reached -2.25782 (best -2.25782), saving model to 'C:\\\\Users\\\\saber\\\\OneDrive\\\\Documents\\\\GitHub\\\\CryptoBot4Dummies\\\\checkpoints\\\\epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'train_loss' was not in top 1\n",
      "Epoch 3, global step 200: 'train_loss' was not in top 1\n",
      "Epoch 4, global step 250: 'train_loss' reached -2.33908 (best -2.33908), saving model to 'C:\\\\Users\\\\saber\\\\OneDrive\\\\Documents\\\\GitHub\\\\CryptoBot4Dummies\\\\checkpoints\\\\epoch=4-step=250-v2.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training complete\n",
      "\n",
      "[Step 3] Precomputing sliding windows on CPU...\n",
      "[Step 3] CPU aggregation complete\n",
      "[Step 3] Preparing global dataset for GPU inference...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": " ",
   "id": "5f4c21036584b2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
